%% This template can be used to write a paper for
%% Computer Physics Communications using LaTeX.
%% For authors who want to write a computer program description,
%% an example Program Summary is included that only has to be
%% completed and which will give the correct layout in the
%% preprint and the journal.
%% The `elsarticle' style is used and more information on this style
%% can be found at
%% http://www.elsevier.com/wps/find/authorsview.authors/elsarticle.
%%
%%
\documentclass[final,3p,times]{elsarticle}

%% Use the option review to obtain double line spacing
%% \documentclass[preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,1p,times]{elsarticle}
%% \documentclass[final,1p,times,twocolumn]{elsarticle}
%% \documentclass[final,3p,times]{elsarticle}
%% \documentclass[final,3p,times,twocolumn]{elsarticle}
%% \documentclass[final,5p,times]{elsarticle}
%% \documentclass[final,5p,times,twocolumn]{elsarticle}

%% if you use PostScript figures in your article
%% use the graphics package for simple commands
%% \usepackage{graphics}
%% or use the graphicx package for more complicated commands
%% \usepackage{graphicx}
%% or use the epsfig package if you prefer to use the old commands
%% \usepackage{epsfig}

\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{inconsolata}
\usepackage{makecell}
\usepackage{jlcode}

\hypersetup{
	unicode=true,
	pdftitle={A descriptive title},
	pdfnewwindow=true,
	colorlinks=true, 	% (false,true)
	pdfborder={0 0 0},
	linkcolor=blue,
	linktoc=all, 		% (none,all)
	citecolor=blue,
	urlcolor=blue,
	breaklinks=false,
}
\renewcommand\theadalign{cl}
\renewcommand\theadfont{\bfseries}
\renewcommand\theadgape{\Gape[4pt]}
% \renewcommand\cellgape{\Gape[4pt]}
\renewcommand{\cellalign}{tl}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers after \end{frontmatter}.
%% \usepackage{lineno}

%% natbib.sty is loaded by default. However, natbib options can be
%% provided with \biboptions{...} command. Following options are
%% valid:

%%   round  -  round parentheses are used (default)
%%   square -  square brackets are used   [option]
%%   curly  -  curly braces are used      {option}
%%   angle  -  angle brackets are used    <option>
%%   semicolon  -  multiple citations separated by semi-colon
%%   colon  - same as semicolon, an earlier confusion
%%   comma  -  separated by comma
%%   numbers-  selects numerical citations
%%   super  -  numerical citations as superscripts
%%   sort   -  sorts multiple citations according to order in ref. list
%%   sort&compress   -  like sort, but also compresses numerical citations
%%   compress - compresses without sorting
%%
%% \biboptions{comma,round}

% \biboptions{}

%% This list environment is used for the references in the
%% Program Summary
%%
\newcounter{bla}
\newenvironment{refnummer}{%
\list{[\arabic{bla}]}%
{\usecounter{bla}%
 \setlength{\itemindent}{0pt}%
 \setlength{\topsep}{0pt}%
 \setlength{\itemsep}{0pt}%
 \setlength{\labelsep}{2pt}%
 \setlength{\listparindent}{0pt}%
 \settowidth{\labelwidth}{[9]}%
 \setlength{\leftmargin}{\labelwidth}%
 \addtolength{\leftmargin}{\labelsep}%
 \setlength{\rightmargin}{0pt}}}
 {\endlist}

\journal{Computer Physics Communications}


\newcommand\WaterLily[0]{\texttt{WaterLily.jl}~}

\begin{document}

\begin{frontmatter}

%% Title, authors and addresses

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for the associated footnote;
%% use the fnref command within \author or \address for footnotes;
%% use the fntext command for the associated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for the associated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%%
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \address{Address\fnref{label3}}
%% \fntext[label3]{}

\title{WaterLily.jl: A differentiable and backend-agnostic Julia solver to simulate incompressible fluid flow and dynamic bodies}

%% use optional labels to link authors explicitly to addresses:
%% \author[label1,label2]{<author name>}
%% \address[label1]{<address>}
%% \address[label2]{<address>}

\author[a]{First Author\corref{author}}
\author[a,b]{Second Author}
\author[b]{Third Author}

\cortext[author] {Corresponding author.\\\textit{E-mail address:} firstAuthor@somewhere.edu}
\address[a]{First Address}
\address[b]{Second Address}

% A submitted program is expected to satisfy the following criteria: it must be of benefit to other physicists, or be an exemplar of good programming practice, or illustrate new or novel programming techniques which are of importance to computational physics community; it should be implemented in a language and executable on hardware that is widely available and well documented; it should meet accepted standards for scientific programming; it should be adequately documented and, where appropriate, supplied with a separate User Manual, which together with the manuscript should make clear the structure, functionality, installation, and operation of the program.

% Your manuscript and figure sources should be submitted through Editorial Manager (EM) by using the online submission tool at \\
% https://www.editorialmanager.com/comphy/.

% In addition to the manuscript you must supply: the program source code; a README file giving the names and a brief description of the files/directory structure that make up the package and clear instructions on the installation and execution of the program; sample input and output data for at least one comprehensive test run; and, where appropriate, a user manual.

% A compressed archive program file or files, containing these items, should be uploaded at the "Attach Files" stage of the EM submission.

% For files larger than 1Gb, if difficulties are encountered during upload the author should contact the Technical Editor at cpc.mendeley@gmail.com.

\begin{abstract}
Integrating computational fluid dynamics (CFD) software into optimization and machine-learning frameworks is hampered by the rigidity of classic computational languages and the slow performance of more flexible high-level languages. WaterLily.jl is an open-source incompressible viscous flow solver written in the Julia language. Solid boundaries are considered through an immersed boundary method that allows to simulate flow past complex geometries with arbitrary motions. The small code base is multidimensional, multi-platform and backend-agnostic (serial CPU, multi-threaded, and GPU execution). Additionally, the dynamically typed language allows the solver to be fully differentiable using automatic differentiation. The computational time per time step scales linearly with the number of degrees of freedom (DOF) on CPUs, and we see up to a 200x speed-up using CUDA kernels resulting in a cost of 1.44 nanoseconds per DOF and time step. This leads to comparable performance with Fortran solvers on many research-scale problems opening up exciting possible future applications on the cutting edge of machine-learning research.
\end{abstract}

\begin{keyword}
computational fluid dynamics; heterogeneous programming; Cartesian-grid methods; Julia; GPU
\end{keyword}

\end{frontmatter}

%%
%% Start line numbering here if you want
%%
% \linenumbers

% All CPiP articles must contain the following
% PROGRAM SUMMARY.

% {\bf PROGRAM SUMMARY/NEW VERSION PROGRAM SUMMARY}
  %Delete as appropriate.
{\bf PROGRAM SUMMARY}
\\
\\
\begin{small}
\noindent
{\em Program Title:} \WaterLily \\
{\em CPC Library link to program files:} (to be added by Technical Editor) \\
{\em Developer's repository link:} \href{https://github.com/WaterLily-jl/WaterLily.jl}{\texttt{https://github.com/WaterLily-jl/WaterLily.jl}} \\
{\em Code Ocean capsule:} (to be added by Technical Editor)\\
{\em Licensing provisions(please choose one):} MIT \\
{\em Programming language:} Julia \\
{\em Supplementary material:} \\
  % Fill in if necessary, otherwise leave out.
% {\em Journal reference of previous version:}*                  \\
  %Only required for a New Version summary, otherwise leave out.
% {\em Does the new version supersede the previous version?:}*   \\
  %Only required for a New Version summary, otherwise leave out.
% {\em Reasons for the new version:*}\\
  %Only required for a New Version summary, otherwise leave out.
% {\em Summary of revisions:}*\\
  %Only required for a New Version summary, otherwise leave out.
{\em Nature of problem(approx. 50-250 words):}\\
  %Describe the nature of the problem here. \\
{\em Solution method(approx. 50-250 words):}\\
  %Describe the method solution here.
{\em Additional comments including restrictions and unusual features (approx. 50-250 words):}\\
  %Provide any additional comments here.

\begin{thebibliography}{0}
\bibitem{1}Program summary reference 1         % This list should only contain those items referenced in the
\bibitem{2}Program summary reference 2         % Program Summary section.
\bibitem{3}Program summary reference 3         % Type references in text as [1], [2], etc.
                               % This list is different from the bibliography at the end of
                               % the Long Write-Up.
\end{thebibliography}
% * Items marked with an asterisk are only required for new versions
% of programs previously published in the CPC Program Library.\\
\end{small}


\section{Introduction}

During the last decade, the computational fluid dynamics (CFD) community has embraced the surge of machine learning (ML) and the new developments in hardware architecture, such as general-purpose GPUs. Hence, classic CFD solvers based on low-level programming languages (C, Fortran) and CPU memory-distributed execution are now adapted to accommodate these new tools. On one hand, the integration of high-level ML libraries and low-level CFD solvers is not straight-forward, aka. the two-language problem \cite{Churavy2022}. When deploying a ML model online with the CFD solver, data exchange is often performed at disk level, significantly slowing down the overall runtime because of disk read/write operations. An improved way to exchange data is performed through memory, either using Unix sockets \cite{Rabault2019, Font2021}, message-passing interface (MPI) \cite{Guastoni2023}, or an in-memory distributed database \cite{Kurz2022,Font2024}, which increases the software complexity. On the other hand, porting classic CFD solvers to GPU is also a non-trivial task which often requires the input and expertise of GPU vendors \cite{Romero2022}. Still, the CFD community has been an active asset in this transition, and it currently offers a rich variety of open-source multi-GPU solvers as summarized in table \ref{tab:solvers}.

\begin{table}[!ht]
\begin{tabular}{llll}
    \hline
    \thead{Name} & \thead{Application} & \thead{Method} & \thead{Language} \\
    \hline
    CANS \cite{Costa2018} & Incompressible canonical flows on Cartesian grids & FDM & Fortran/OpenACC \\
    GAL{\AE}XI \cite{Kempf2024} & Compressible flows on unstructured grids & DG & CUDA-Fortran \\
    nekRS \cite{Fischer2022} & Incompressible flows on unstructured grids & SEM & C++/OCCA \\
    Oceananigans.jl \cite{Ramadhan2020} & Geophysical flows & FVM & Julia \\
    OpenSBLI \cite{Lusher2021} & \makecell{Code-generation system for compressible flows\\ on structured grids} & FDM & \makecell{Python +\\ CUDA/OpenCL} \\
    PyFR \cite{Witherden2015} & Compressible flows on unstructured grids & FR & \makecell{Python + C/OpenMP, \\ CUDA, OpenCL} \\
    RHEA \cite{Jofre2023} & Compressible flows on Cartesian grids & FDM & C++/OpenACC \\
    SOD2D \cite{Gasparino2024} & \makecell{Compressible/incompressible flows on \\ unstructured grids} & SEM & Fortran/OpenACC \\
    STREAmS \cite{Bernardini2021} & \makecell{Compressible canonical wall-bounded flows \\ on Cartesian grids} & FDM & CUDA-Fortran \\
    \hline
\end{tabular}
\caption{Examples of multi-GPU open-source CFD solvers. Methods are abbreviated as: finite difference method (FEM), discontinous Galerkin (DG), spectral element method (SEM), finite volume method (FVM), and flux reconstruction (FR).}\label{tab:solvers}
\end{table}

In this context, Julia \cite{Bezanson2017} emerges as an open-source, compiled, dynamic, and composable programming language specifically designed for scientific computing which can help tackle such software challenges. High-level libraries and low level code can co-exist without compromising computing performance. Moreover, its excellent meta-programming capabilities, dynamic types, and multiple-dispatch strategy maximizes code re-usability. A great example of this is the \texttt{KernelAbstractions.jl} library \cite{Churavy2023}, which enables writing heterogeneous kernels for different backends (multi-threaded CPU, NVIDIA, AMD, and others) in a single framework.
Julia has been also tested in many HPC systems, and the reader is referred to \cite{Churavy2022} for a comprehensive review.

In this work, we introduce a new CFD solver with heterogeneous execution written in Julia, namely \\ \href{https://github.com/WaterLily-jl/WaterLily.jl}{\WaterLily}. Differently to the solvers detailed in table \ref{tab:solvers}, \WaterLily profits from a dynamically-typed language that allows to efficiently implement performance-critical code in a compact and uniform framework, noting that the solver codebase is less that 1000 lines of code. This results in a fully-differentiable CFD solver that is easy to maintain and that can run in CPU or GPU architectures of different vendors without compromising performance. With this, the numerical methods and software design are respectively reported in sections \S\ref{sec:numerical_methods} and \S\ref{sec:software_design}. The solver is benchmarked and validated in \S\ref{sec:benchmark_validation}. Two different test cases showcasing notable features of the solver are shown in \S\ref{sec:applications}. And, finally, the discussion, expectations, and conclusions are presented in \S\ref{sec:conclusions}.

\section{Numerical methods}\label{sec:numerical_methods}
WaterLily uses Boundary Data Immersion Method (BDIM) to simulate the fluid flow around immersed bodies \cite{Weymouth2011,Maertens2014,Lauber2022}. The preceding references give the precise mathematical formulation, as well as detailed validation of the immersed-boundary method's accuracy. To summarise the approach, the momentum equation defined over the fluid domain
\begin{equation}
    \mathcal{\dot F}:\ \dot u_i = -p_{,i} - (u_i u_j)_{,j}+\nu u_{i,jj} \quad \forall i,j \in 1\ldots n
\end{equation}
is integrated in time and convolved with a prescribed body velocity defined over the solid domain 
\begin{equation}
    \mathcal{B}:\ u_i = V_i \quad \forall i \in 1\ldots n
\end{equation}
resulting in a single meta-equation valid over the whole space. In these equations, $u_i$ are the velocity components in a $n$-dimensional flow, $p$ is the pressure scaled by the fluid density, $\nu$ is the fluid viscosity, $V_i$ is the body velocity, indices after commas indicate spacial derivatives, and summation is used over repeated indices. As with these equations, WaterLily can be applied to simulations of any number of dimensions $n$, although we typically restrict applications to 2D and 3D flows.

The immersed-boundary thickness $\epsilon$ defines the region directly affected by the prescribed body velocities, but the flow inside this region still obeys the fluid dynamic equations with second-order accuracy \cite{Maertens2014}. The transition between these two regions is defined by the properties of the immersed surface, specifically the signed-distance $d$ \& normal $\hat n$ from any point in space to the closest surface point. This, along with the body velocity $V_i$, defines the local meta-equation.

WaterLily implements the governing equation using a finite-volume approach on a uniform Cartesian grid with staggered velocity-pressure variable placement. Since all grid cells are identical, no grid information is stored. Second-order central differences are used for the pressure and diffusion terms, while a flux-limited Quick scheme is used on the convective term. While explicitly turbulence models have been used for specific projects, the core WaterLily solver is model-free, making it an implicit Large Eddy Simulation (iLES) solver \cite{Margolin2006}.

Finally, the momentum equation is integrated in time using with an explicit predictor-corrector update scheme \cite{Lauber2022}. The velocity is restricted to be incompressible (divergence-free $u_{i,i}=0$) using a pressure projection scheme at each step. The resulting Poisson equation has spatially varying coefficient in the presence of immersed boundaries, and is solved using a Geometric Multi-Grid method \cite{Weymouth2022}. The time step is adapted automatically based on the maximum CFL number in the domain. 

\section{Software design}\label{sec:software_design}

Julia’s flexible and fast programming capabilities enabled the implementation of \WaterLily to have many special features in a minimal codebase (approximately 1000 lines of code). For example, automatic differentiation (AD) is used to define all the properties of the immersed geometry from a user-defined signed-distance function and a coordinates-mapping function. Moreover, the whole solver is also differentiable based on AD. This allows solving optimization problems related to the immersed body with reduced computational cost compared to a finite-difference/sampling approach. In addition, the AD can also be used to develop accelerated data-driven GMG methods as demonstrated in \cite{Weymouth2022}.

The most important Julia features for implementing the solver to run on heterogeneous backends are (i) the dynamic typing system, (ii) the meta-programming capabilities, and (iii) the rich open-source packages. Multiple-dispatch enables simple functions (such as broadcasting or reductions operations on arrays) to be written at high-level by the user, while intermediate Julia libraries, and ultimately the compiler, will specialize the code for efficient execution on a particular architecture (CPU or GPU). For more specialized tasks, \WaterLily uses Julia's meta-programming features to generate code that produces an individual kernel for the specific task. The kernel can be used to offload the computational work into a GPU, or to run it in a multi-threaded CPU environment depending on the available system architecture. As an example, the gradient of the \jlinl{n}-dimensional pressure field \jlinl{p} is applied to the velocity field \jlinl{u} as follows
\begin{jllisting}
for i in 1:n  # apply pressure gradient
    @loop u[I, i] -= c[I, i] * (p[I] - p[I - ∂(i)]) over I in inside(p)
end
\end{jllisting}
where \jlinl{∂(i)} is a function defining a Cartesian index step in the direction \jlinl{i}, \jlinl{c} are the coefficients in the pressure-Poisson matrix arising from the discretization scheme, and \jlinl{inside(p)} provides the range of Cartesian indices \jlinl{I} in the pressure field to loop over (excluding ghost cells). For example, if \jlinl{size(p)==(10, 10)}, then \jlinl{inside(p)} yields a range of \jlinl{CartesianIndices((2:9, 2:9))}. When applying the \jlinl{@loop} macro to this expression, the following kernel is produced based on the \texttt{KernelAbstractactions.jl} package \citep{Churavy2023}
\begin{jllisting}
@kernel function kern_(u, c, p, i, @Const(I0)) # automatically generated kernel
    I = @index(Global, Cartesian)
    I += I0
    @fastmath @inbounds u[I, i] -= c[I, i] * (p[I] - p[I - ∂(i)])
end
\end{jllisting}
which is subsequently launched with the auto-generated call
\begin{jllisting}
kern_(get_backend(u))(u, c, p, i, inside(p)[1] - oneunit(inside(p)[1]), ndrange=size(inside(p)))
\end{jllisting}

Note that \jlinl{@kernel}, \jlinl{@index}, \jlinl{@Const} and \jlinl{get_backend} are part of the \texttt{KernelAbstractactions.jl} API and ultimately generate the appropriate kernel based on the backend inferred by \jlinl{get_backend(u)}. Also note that a Cartesian-index based parallelization across the global memory is used, and that the \jlinl{@Const(I0)} argument passes the ghost-cell offset information into the kernel. The workgroup size for the parallelization of the range of Cartesian indices (\jlinl{ndrange}) is automatically inferred base on the size of each dimension in \jlinl{ndrange}. Moreover, the backend of the working arrays, such as \jlinl{u} or \jlinl{p}, is specified by the user through the \jlinl{mem} (for memory) keyword argument when creating a \jlinl{Simulation} object. Hence, with a simple flag, the CFD simulation can be run on a CPU or a GPU from different vendors. Currently, \WaterLily has been successfully tested on both NVIDIA and AMD GPUs. Similarly, the precision of the simulation is specified with the keyword argument \jlinl{T}, which for example can be set to \jlinl{Float32} (single) or \jlinl{Float64} (double) precision.

As hinted, the main component in \WaterLily is the \jlinl{Simulation} type, which holds information about the fluid through the \jlinl{Flow} type, and the immersed body (or bodies) through the \jlinl{AutoBody} type. Hence, to set up a simulation, the user must specify the size of the Cartesian grid as well as other optional properties such as characteristic length and velocity, fluid viscosity, and type of boundary conditions (slip by default, otherwise a convective outlet or a periodic condition can be selected too). On the other hand, the \jlinl{AutoBody} type holds the signed-distance function as well as the coordinates mapping for moving boundaries. More detailed examples on how to set up a simulation are available in \S\ref{sec:applications}.

\section{Benchmark and validation} \label{sec:benchmark_validation}
\begin{itemize}
  \item Architectures: NVIDIA H100 (Marenostrum 5 at BSC), AMD Radeon Instinct MI50 (CTE-AMD cluster at BSC), serial execution (?)
  \item Cylinder case is not going turbulent with periodic BCs
  \item cost on AMDGPU
  \item Validation on sphere BiotSavart
\end{itemize}

The performance of the solver is assessed on three different cases with increasing level of complexity: the Taylor--Green vortex (TGV) at $Re=1600$ [ref], flow past a fixed sphere at $Re=3700$, and flow past a moving circular cylinder at $Re=1000$. The flow topology is shown in figure [ref]. Note that the cases range from a flow free of solid boundaries to a flow containing a dynamic body. The TGV case consists of a developing flow transitioning to turbulence in a $L^3$ triple-periodic cubic domain. The initial condition for the velocity vector field $\vec{u}_0$ is prescribed as
\begin{align}
u_0 &= -U\sin(\kappa x)\cos(\kappa y)\cos(\kappa z) \\
v_0 &= U\cos(\kappa x)\sin(\kappa y)\cos(\kappa z) \\
w_0 &= 0,
\end{align}
where $U=1$ is the characteristic velocity and $\kappa=2\pi/L$ is the wavenumber. Similarly to \cite{Dairay2017}, we use the half-domain defined by the characteristic length as the effective computational domain, and apply symmetry boundary conditions to lower the cost of the simulation. To test different grid resolutions, we select $L=\left\{2^6,2^7,2^8,2^9\right\}$ resulting into grids of 0.26, 2.10, 16.78, and 134.22 million of degrees of freedom (DOF), respectively.

With respect to the sphere case, its diameter is taken as the characteristic length, and a $16L\times6L\times6L$ domain is defined for increasing resolutions of $L=\left\{2^3,2^4,2^5,2^6\right\}$ resulting in 0.29, 2.36, 18.87, and 150.99 million DOF, respectively. For the circular cylinder, the diameter is again taken as the characteristic length in a $12L\times6L\times2L$ domain and resolutions of $L=\left\{2^4,2^5,2^6,2^7\right\}$ resulting in 0.59, 4.72, 37.75, 301.99 million DOF grids are considered. We note that the finer cylinder grid consumes 39 GB of memory using single precision, and hence can be fitted in the NVIDIA Hopper H100 64GB used for benchmarking. Both the sphere and cylinder cases are initialized with a uniform flow condition $\vec{u}_0=(U,0,0)$, slip conditions are applied on the lateral boundaries, and a convective outlet condition is applied at the downstream plane.

\begin{figure}[!ht]
  \centering
  \begin{subfigure}[t]{0.33\linewidth}
      \centering
      \includegraphics[width=\linewidth]{img/tgv_benchmark.pdf}
      \caption{TGV}
  \end{subfigure}
  \begin{subfigure}[t]{0.33\linewidth}
    \centering\hspace*{-0.2cm}
    \includegraphics[width=\linewidth]{img/sphere_benchmark.pdf}
    \caption{Sphere}
  \end{subfigure}
  \begin{subfigure}[t]{0.33\linewidth}
    \centering\hspace*{-0.2cm}
    \includegraphics[width=\linewidth]{img/cylinder_benchmark.pdf}
    \caption{Moving cylinder}
  \end{subfigure}
	\caption{Time to run 100 time steps in single precision for the Taylor--Green vortex (left), fixed sphere (center), and moving cylinder (right) cases at different grid sizes. The CPU execution comprises multiple number of threads from single thread (serial) to 16 threads (multi-threading). The speedup for each case with respect to the serial execution (CPUx1) is shown above each bar. The speedup is computed as time(CPUx1)/time(X). The benchmarks have been run on an accelerated node of the Marenostrum5 supercomputer using Intel Xeon Platinum 8460Y @ 2.3GHz cores and a NVIDIA Hopper H100 64GB HBM2 GPU.}
	\label{fig:benchmarks}
\end{figure}

\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.4\linewidth]{img/cost.pdf}
\caption{Cost defined as execution time per grid DOF and time step on the different cases and grid levels. Data resulting from figure \ref{fig:benchmarks} benchmarks.}
\label{fig:cost}
\end{figure}

\begin{figure}[!ht]
  \centering
  \begin{subfigure}[t]{0.33\linewidth}
      \centering
      \includegraphics[width=\linewidth]{img/tgv_profile.pdf}
      \caption{TGV}
  \end{subfigure}
  \begin{subfigure}[t]{0.33\linewidth}
    \centering
    \includegraphics[width=\linewidth]{img/sphere_profile.pdf}
    \caption{Sphere}
  \end{subfigure}
  \begin{subfigure}[t]{0.33\linewidth}
    \centering
    \subcaptionbox{Moving cylinder\hspace*{5em}}{%
      \includegraphics[width=\linewidth]{img/cylinder_profile.pdf}%
    }
  \end{subfigure}
  \caption{Kernel timings distribution for the 3rd-level grids of the different test cases. Timings are measured as the median value of the kernel execution time for 1000 time steps, noting that each kernel can be called more than once for each time step (ie. predictor-corrector scheme). We note that field \texttt{BC!} accounts for all the following the boundary-conditions subroutines: \texttt{BC!}, \texttt{BDIM!}, \texttt{BCTuple}, \texttt{exitBC!}, where the latter is the most expensive one. Tests are conducted using single precision in an NVIDIA GeForce RTX 4060 Laptop GPU. During the tests, 99.9\% of time is spent in kernel execution while only 0.1\% is spent on device-to-host memory-copy calls (related to the pressure solver).}
\label{fig:profiling}
\end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.8\linewidth]{img/tgv.pdf}
	\caption{Taylor--Green vortex (TGV) validation. Direct numerical simulation (DNS) data from \cite{Dairay2017} is used as reference.}
	\label{fig:tgv}
\end{figure}

\section{Sample applications}\label{sec:applications}
Three applications are selected to demonstrate the capability of the package to analyze general fluid flows. The examples also showcase the advantages of a differentiable back-end agnostic Cartesian-grid solver.

\subsection{Optimized control cylinders}

\begin{figure}
    \centering
    \begin{subfigure}[t]{0.8\linewidth}
        \centering
        \includegraphics[width=\linewidth,trim={50 70 20 210},clip]{img/SpinCylFlood.pdf}
        \vspace{-1cm}
        \caption{Vorticity field}
    \end{subfigure}
    \begin{subfigure}[b]{0.35\linewidth}
        \includegraphics[width=\linewidth]{img/SpinCylHist.pdf}
        \vspace{-1cm}
        \caption{Scaled power history}
    \end{subfigure}\hspace{20pt}
    \begin{subfigure}[b]{0.35\linewidth}
        \centering
        \includegraphics[width=\linewidth]{img/SpinOptim.pdf}
        \vspace{-1cm}
        \caption{Optimization process}
    \end{subfigure}
    \caption{Controlled flow over a static cylinder using spinning cylinders in the wake. The small cylinder (purple) has scale spin velocity $\xi$, driving the flow to be symmetry and steady after an initial transient, as measured by the vorticity (a) and the scaled power coefficient $C_P$ (b). The net propulsive efficiency is optimized using the differentiable solver (c).}
    \label{fig:spinning_circle}
\end{figure}

The first example will be optimizing the controlled 2D flow around a circle using a pair of small spinning circles placed 120 degrees relative to the inflow direction, Fig~\ref{fig:spinning_circle}a. Experimental and numerical studies of this system have show the capability of the spinning cylinders to control the flow over the large circle \cite{schulmeister2017}, establishing a steady symmetric wake, reducing the system drag and even producing a net thrust as the rotation rate is increased.

The system is described by a few dimensionless ratios: $Re=UD/\nu$ the Reynolds number based on the large circle diameter and inflow velocity, as well as $d/D$, $g/D$ and $\xi=\frac 12 d\Omega/U$ the scaled diameter of the control circle, it's gap from the large circle surface, and it's scaled surface speed. This system is simulated with WaterLily using the values of $Re=500,\ d/D=0.15,\ g/D=0.05$ with grid resolution $D/h=96$. The domain is sized to $(6D,2D)$ taking advantage of the known symmetry of the flow by using a symmetry plane and only modelling the upper half of the full domain. 

The entire differentiable simulation is defined with the simple script:
\begin{jllisting}
using WaterLily: norm2                   # vector length
rot(θ) = [cos(θ) -sin(θ); sin(θ) cos(θ)] # rotation matrix
function drag_control_sim(ξ;D=96,Re=500,d_D=0.15f0,g_D=0.05f0)
    # set up big cylinder
    C,R,U = [2D,0],D÷2,1
    big = AutoBody((x,t)->norm2(x-C)-R)         # signed-distance function
    
    # set up small control cylinder
    r = d_D*R; c = C+(R+r+g_D*D)*[1/2,√3/2]
    small = AutoBody((x,t)->norm2(x)-r,         # signed-distance function
                     (x,t)->rot(ξ*U*t/r)*(x-c)) # center and spin!
    
    # set up simulation
    Simulation((6D,2D),(U,0),D;ν=U*D/Re,body=big+small,T=typeof(ξ))
end
\end{jllisting}
This example demonstrates that WaterLily can combine AutoBodies based on the arithmetic of signed-distance functions. The two circles \texttt{big} and \texttt{small} are defined with a line of code each and combined trivially with \texttt{body=big+small}. It also demonstrates using a variable $\xi$ to set the types used for the simulation. This allows easy switching between any floating point precision, but it also allows automatic differentiation to be applied to the solver as a whole by running the code with a \texttt{T=Dual} data-type holding the value and derivative simultaneously \cite{RevelsLubinPapamarkou2016}.

We use the differentiable solver to maximize the scaled propulsive power $C_P = FU/\rho dc^3$ where $F$ is the net thrust force on the system. This metric is  proportional to the propulsive efficiency since $\rho dc^3$ scales with the power required to rotate the control cylinders \cite{schulmeister2017}. The time history of $C_P$ is plotted for a few values of $\xi$ in Fig~\ref{fig:spinning_circle}b, demonstrating that only a few convective cycles are required to reach steady state, as well as the control authority of $\xi$ over the propulsive power.

We optimize $\hat\xi=\text{argmax}\ C_P(\xi)$ at time $t^*=tU/L=2$ using Davidon's method \cite{davidon1991}, which evaluates $C_P$ and it's derivative $\partial C_P/\partial \xi$ at points bracketing an optimum, using inverse cubic interpolation to iteratively restrict the interval. Both the value and derivative of the power are computed simultaneously using dual numbers, at a cost only 80\% larger than evaluating the function alone. Fig~\ref{fig:spinning_circle}c shows the resulting evaluation history starting with the interval $\xi=[3,8]$, leading to the optimum $\hat\xi\approx 6.26$ in a few iterations. Rates above this optimum produce more net thrust, but require excessive rotation rates to produce.

\subsection{Deforming and dynamic geometries}


\begin{figure}
    \centering
    \includegraphics[width=0.24\linewidth,trim={90 150 70 200},clip]{img/Jelly_1.png}
    \includegraphics[width=0.24\linewidth,trim={90 150 70 200},clip]{img/Jelly_2.png}
    \includegraphics[width=0.24\linewidth,trim={90 150 70 200},clip]{img/Jelly_3.png}
    % \includegraphics[width=0.16\linewidth,trim={90 150 70 200},clip]{img/Jelly_4.png}
    \includegraphics[width=0.24\linewidth,trim={90 150 70 200},clip]{img/Jelly_5.png}
    % \includegraphics[width=0.16\linewidth,trim={90 150 70 200},clip]{img/Jelly_6.png}

    \caption{Flow induced by a pulsing jellyfish geometry visualized by vorticity magnitude at equally spaced intervals over a cycle.}
    \label{fig:jelly}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.32\linewidth,trim={90 150 70 200},clip]{img/Butterfly_1.png}
    \includegraphics[width=0.32\linewidth,trim={90 150 70 200},clip]{img/Butterfly_2.png}
    \includegraphics[width=0.32\linewidth,trim={90 150 70 200},clip]{img/Butterfly_3.png}
    \includegraphics[width=0.32\linewidth,trim={90 150 70 200},clip]{img/Butterfly_4.png}
    \includegraphics[width=0.32\linewidth,trim={90 150 70 200},clip]{img/Butterfly_5.png}
    \includegraphics[width=0.32\linewidth,trim={90 150 70 200},clip]{img/Butterfly_6.png}

    \caption{Flow induced by a flapping butterfly geometry visualized by vorticity magnitude at equally spaced intervals over a cycle.}
    \label{fig:butterfly}
\end{figure}
The final two examples showcase the solvers ability to handle more complex geometries with ease. The first is a pulsing jellyfish-inspired geometry, and the second is a flapping butterfly-inspired geometry. Both of these cases are fast enough to simulate on a laptop GPU for live demonstrations.

\begin{jllisting}
function jelly(p=5;Re=5e2,mem=Array,U=1)
    # Define simulation size, geometry dimensions, & viscosity
    n = 2^p; R = 2n/3; h = 4n-2R; ν = U*R/Re

    # Motion functions
    ω = 2U/R
    A(t) = 1 .- [1,1,0]*0.1*cos(ω*t)
    B(t) = [0,0,1]*((cos(ω*t)-1)*R/4-h)
    C(t) = [0,0,1]*sin(ω*t)*R/4

    # Build jelly from a mapped sphere and plane
    sphere = AutoBody((x,t)->abs(√sum(abs2,x)-R)-1, # sdf
                      (x,t)->A(t).*x+B(t)+C(t))     # map
    plane = AutoBody((x,t)->x[3]-h,(x,t)->x+C(t))
    body =  sphere-plane

    # Return initialized simulation
    Simulation((n,n,4n),(0,0,-U),R;ν,body,mem,T=Float32)
end
\end{jllisting}

The bell of the jellyfish is constructed with more AutoBody-arithmetic, in this case taking the difference of a hollow sphere with an oriented plane. This geometry is made to pulse by mapping the coordinates harmonically in the radial and transverse directions. While the geometry maintains a roughly constant solid volume throughout the pulse, small deviations are handled gracefully by the solver. Fig~\ref{fig:jelly} shows equally spaced snapshots of the geometry and resulting flow throughout the cycle. Each cycle generates a strong propulsive vortex ring which breaks up as it propagates away, in qualitative agreement with experimental studies \cite{dabiri2005}.

The butterfly geometry is a pair of membrane wings defined using the ParametricBodies package \cite{WeymouthLauber2023}. The wing's planform is defined by a set of planar points which are interpolated using a cubic spline. The 3D distance function and normal to this planar membrane are then evaluated using a parametric root-finding method to immerse the geometry in the simulation as with the examples above. Harmonic coordinate rotations about the "shoulder" are used to flap the wings. Fig~\ref{fig:butterfly} shows equally spaced snapshots of the geometry and resulting flow throughout the cycle. The classic clap-and-fling vortex formations are observed \cite{weis1973}.

\section{Conclusions}\label{sec:conclusions}

\section{Acknowledgements}\label{sec:acknowledgements}
\begin{itemize}
    \item BSC HPC services
    \item Dr. Lucas Gasparino, Valentin Churavy
\end{itemize}

\bibliographystyle{elsarticle-num}
\bibliography{main.bib}

\end{document}

%%
%% End of file